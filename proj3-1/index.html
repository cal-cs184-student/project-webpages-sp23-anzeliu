<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <style>
        body {
            background-color: white;
            padding: 100px;
            width: 1000px;
            margin: auto;
            text-align: left;
            font-weight: 300;
            font-family: 'Open Sans', sans-serif;
            color: #121212;
        }
        
        h1,
        h2,
        h3,
        h4 {
            font-family: 'Source Sans Pro', sans-serif;
        }
        
        kbd {
            color: #121212;
        }
    </style>
    <title>CS 184 Path Tracer</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

    <script>
        MathJax = {
            tex: {
                inlineMath: [
                    ['$', '$'],
                    ['\\(', '\\)']
                ]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>


<body>

    <h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
    <h1 align="middle">Project 3-1: Path Tracer</h1>
    <h2 align="middle">YOUR NAME(S)</h2>

    <!-- Add Website URL -->
    <h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/project-webpages-sp23-anzeliu/proj3-1/index.html">project-webpages-sp23-anzeliu</a></h2>

    <br><br>

    <div align="center">
        <table style="width=100%">
            <tr>
                <td align="middle">
                    <img src="images/example_image.png" width="480px" />
            </tr>
        </table>
    </div>

    <div>

        <h2 align="middle">Overview</h2>
        <p>
            In project 3-1 Path Tracer, I implemented a physically-based renderer using a path tracing algorithm which starts with camera ray generation, ray-scene intersection, Bounding Volume Hierarchy acceleration structure, physically based direct and indirect
            lighting, and adaptive sampling. First, I generated num_samples camera rays, traced them through the scene, and computed the average of the returned radiance. Second, I implemented the BVH acceleration structure to accelerate the manipulation
            and intersection test between a ray and the primitives held by a BVH node. Ray-bounding box intersection is also implemented using the methods of ray-plane intersection for axis-aligned plane and ray intersection with axis-aligned box. Next,
            I generated direct illumination with zero bounce and one bounce radiance, followed by generating indirect illumination with multiple bounces: light from the light source intersects with multiple surfaces in the scene before reaching the camera.
            Finally, I implemented adaptive sampling to reduce the number of samples for pixels that converge faster with low sample rate. By modifying various parameters for rendering, for example, the number of camera rays per pixel, number of samples
            per area light, and maximum ray depth, we can achieve different levels of denoising on the rendered images. The effect of each of the methods introduced above on the rendered scenes are analyzed in this write-up.

        </p>
        <br>

        <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
        <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

        <h3>
            Walk through the ray generation and primitive intersection parts of the rendering pipeline.
        </h3>
        <p> Ray generation
        </p>
        <p>
            Randomly sample positions within a pixel at image coordinate (x, y). For each sample, compute the coordinate in camera space, using the coordinates of the axis-aligned rectangular virtual camera sensor. Transform the ray to be in world space using camera-to-world
            rotation matrix and the camera position in world space. Set the ray’s min_t to nClip and max_t to fClip. Call the PathTracer function est_radiance_global_illumination on each ray and take the average of all the resulting radiances, which is
            a Monte Carlo approximation of the integral of radiance over the pixel. Finally, update the pixel at (x, y) of the sample buffer with the computed average.
        </p>
        <p>Primitive Intersection</p>
        <p>
            Primitives, which are triangles and sphere, intersect with camera rays. Knowing whether there is an intersection and where exactly the intersection is are necessary to render pixels with the specific radiance. Ray-triangle intersection algorithm is explained
            in the next part and ray-sphere intersection is explained below.
        </p>
        <p>
        </p>
        <p>To compute the ray-sphere intersection, first implement Sphere::test() where we substitute the ray equation in the sphere equation and quadratic formula to calculate the intersection t. There are three different types of results from the quadratic
            formula: 1) complex roots, 2) real and equal roots, 3) real and distinct roots. When the results of the quadratic formula are complex roots, there is no intersection between the ray and the sphere. When the results of the quadratic formula
            are real and equal roots, the ray is tangent to the sphere. When the results of the quadratic formula are real and distinct roots, the ray cuts through the sphere. Therefore, the ray-sphere intersection is only valid if the quadratic formula
            results in real and equal roots or real and distinct roots. Second, implement Sphere::has_intersection() which calls Sphere::test() and further checks if the smaller of the two possible intersections – the two roots, is in between min_t and
            max_t of the ray. If it is, then the intersection is valid. Finally, implement Sphere::intersect() which calls has_intersection() – return false if has_intersection() returns false, as well as updates the members of the intersection struct
            with time t, surface normal n, primitive, and surface material bsdf.
        </p>
        <br>

        <h3>
            Explain the triangle intersection algorithm you implemented in your own words.
        </h3>
        <p>
            To compute the ray-triangle intersection, first implement the Möller-Trumbore algorithm, from which we can find the time t of intersection and the barycentric coordinates. To test if the time t of intersection is valid, check if t is between min_t and
            max_t. To test if the intersection point is inside the triangle, check if each of the barycentric coordinates is between 0 and 1. If the barycentric coordinates do not satisfy the condition, the point intersecting with the plane the triangle
            is on is actually not inside the triangle, in which case, false is returned. Otherwise, in the case of a valid ray-triangle intersection, update all members of the intersection struct, isect, with time t, surface normal n, primitive, and surface
            material bsdf.
        </p>
        <br>

        <h3>
            Show images with normal shading for a few small .dae files.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part1/cube.png" align="middle" width="400px" />
                        <figcaption>cube.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part1/CBempty.png" align="middle" width="400px" />
                        <figcaption>CBempty.dae</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part1/CBspheres.png" align="middle" width="400px" />
                        <figcaption>CBspheres_lambertian.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part1/gems.png" align="middle" width="400px" />
                        <figcaption>CBgems.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>


        <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
        <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

        <h3>
            Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
        </h3>
        <p>
            To construct BVH, first construct the bounding box of a list of primitives.Next, initialize a BVHNode with the bounding box. Sort the list of primitives based on the centroid of their bounding box along the x, y, or z axis. Specifically, while iterating
            through the list of primitives, in addition to building and expanding the bounding box, declare another bounding box instance, centroid_hull, which would be the convex hull of the centroids of the primitive bounding box. Call centroid_hull.expand(bb.centroid()),
            where bb is the bounding box of a primitive, in the iteration loop. The extent of the centroid_hull is used to determine along which axis the centroids of primitive bounding boxes are compared, which in turn determines the sorting of the list
            of primitives at that BVHNode. If the x value of the centroid_hull.extent is greater than the y and z values, then the x values of the centroids are compared and the primitives are sorted based on their bounding box’s centroid along the x
            axis, in ascending order. Similarly, if the y value of the centroid_hull.extent is greater than the x and z values, then the primitives are sorted based on their bounding box’s centroid along the y axis, in ascending order. Similar case for
            sorting along the z axis. Once sorting completes, the start iterator now points to the beginning of the sorted list of primitives and the end iterator points to the end of the sorted list.
        </p>
        <p>
            Next, determine whether the node is an interior node or a leaf node based on the number of primitives: if there are no more than max_leaf_size primitives, the newly created node is a leaf node and update its start and end to the beginning of the sorted
            list and the end of the sorted list respectively. Otherwise, the node is an interior node, for which we find the middle point in the list of primitives and set it as the bound. Finally, recursively call construct_bvh for node->l and node->r,
            one for primitives from start to bound and another for primitives from bound to end. The half of the primitives with bbox centroid smaller than the median primitive’s bbox centroid along a specific axis are in node->l, and the other half are
            in node->r. The node is returned.
        </p>
        <p>Heuristic chosen for picking the splitting point is the median of the bounding box centroids along axis x, y, or z depending on the extent of the convex hull of the centroids of the primitive bounding box. If the extent of the convex hull is longer
            along the x axis, then the x values of the bbox centroid coordinates are used to compare and sort the list of primitives. Half of the primitives with bbox centroid x value smaller than that median x value are grouped to the left node and the
            other half with bbox centroid x value greater than the median x value are grouped to the right node. This helps to construct the BVH efficiently as there are always equal numbers of primitives splitted into left and right. The memory is efficiently
            used; the split occurs based on the longest axis of the convex hull extent, the primitives in each node are closer together coordinate-wise, thus locality is exploited.
        </p>

        <h3>
            Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part2/maxplanck.png" align="middle" width="400px" />
                        <figcaption>maxplanck.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part2/CBlucy.png" align="middle" width="400px" />
                        <figcaption>CBlucy.dae</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part2/dragon.png" align="middle" width="400px" />
                        <figcaption>dragon.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part2/wall-e.png" align="middle" width="400px" />
                        <figcaption>wall-e.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>

        <h3>
            Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
        </h3>
        <p>
            Running local on computer and with 8 threads, the render times for three scenes with moderately complex geometries with and without BVH acceleration are shown in the following table:
        </p>
        <div align="middle">
            <img src="images/part2/render_time.png" align="middle" width="950px" />
        </div>
        <p>
            Render time increases for both with and without BVH acceleration as the number of primitives increases. The render time with BVH acceleration is less than 1% of the render time without BVH acceleration. The splitting heuristics is the median centroid
            of the primitive bounding box. This helps to construct the BVH efficiently as there are always equal numbers of primitives splitted into left and right. The memory is efficiently used; the split occurs based on the longest axis of the convex
            hull extent, the primitives in each node are closer together coordinate-wise, thus locality is exploited.
        </p>
        <br>

        <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
        <!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

        <h3>
            Walk through both implementations of the direct lighting function.
        </h3>
        <p>
            BRDF stands for bidirectional reflectance distribution function, which represents the ratio of light reflected into an outgoing direction from a surface to the light from incoming direction, which then determines the brightness of the surface. Note that
            by convention, the incoming direction vector points away from the surface. Lambertian BRDF is a model of diffuse reflectance and is invariant to the viewing direction. Lambertian BRDF is equal to reflectance divided by pi.
        </p>

        <p>
            Direct lighting includes both the zero bounce illumination and the one bounce illumination. Zero bounce illumination is the light from the light source that directly reaches the camera without intersecting with any objects in the scene. Zero bounce radiance
            is obtained by the emissive spectrum of the surface material. One bounce illumination involves casting a camera ray through a pixel into the scene and determining the color of the pixel if the ray intersects with some primitives. Once bounce
            radiance can be implemented with two different sampling methods: uniform hemisphere sampling and importance sampling lights.
        </p>
        <h3>
            Uniform Hemisphere Sampling
        </h3>
        <div align="middle">
            <img src="images/part3/hemisphere.png" align="middle" width="950px" />
        </div>
        <h3>
            Importance Sampling Lights
        </h3>
        <div align="middle">
            <img src="images/part3/importance.png" align="middle" width="950px" />
        </div>
        <p>Finally in one_bounce_radiance, call estimate_direct_lighting_hemisphere or estimate_direct_lighting_importance depending on direct_hemisphere_sample.
        </p>
        <br>
        <h3>
            Show some images rendered with both implementations of the direct lighting function.
        </h3>

        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <!-- Header -->
                <tr align="center">
                    <th>
                        <b>Uniform Hemisphere Sampling</b>
                    </th>
                    <th>
                        <b>Light Sampling</b>
                    </th>
                </tr>
                <br>
                <tr align="center">
                    <td>
                        <img src="images/part3/3_CBspheres_hemisphere.png" align="middle" width="400px" />
                        <figcaption>CBspheres_lambertian.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part3/3_CBspheres_lighting.png" align="middle" width="400px" />
                        <figcaption>CBspheres_lambertian.dae</figcaption>
                    </td>
                </tr>
                <br>
                <tr align="center">
                    <td>
                        <img src="images/part3/3_CBbunny_hemisphere.png" align="middle" width="400px" />
                        <figcaption>CBbunny.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part3/3_CBbunny_lighting.png" align="middle" width="400px" />
                        <figcaption>CBbunny.dae</figcaption>
                    </td>
                </tr>
                <br>
            </table>
        </div>
        <br>

        <h3>
            Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling,
            <b>not</b> uniform hemisphere sampling.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part3/3_CBspheres_l1.png" align="middle" width="400px" />
                        <figcaption>1 Light Ray (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part3/3_CBspheres_l4.png" align="middle" width="400px" />
                        <figcaption>4 Light Rays (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part3/3_CBspheres_l16.png" align="middle" width="400px" />
                        <figcaption>16 Light Rays (CBspheres_lambertian.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part3/3_CBspheres_l64.png" align="middle" width="400px" />
                        <figcaption>64 Light Rays (CBspheres_lambertian.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>
            For the scene with one area light and two spheres, as the number of light rays increases, the noise in soft shadows decreases. If we only take a few light samples of the area light, the rendering engine could not pick up enough lighting thus could not
            compute enough radiance to determine the specific brightness of the object surface, causing a grainy effect in the rendered image.
        </p>
        <br>

        <h3>
            Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
        </h3>
        <p>
            Uniform hemisphere sampling produces more noisy renders than lighting sampling. Light sampling method samples each light in the scene directly with different pdfs instead of uniformly sampling a direction in a hemisphere. With light sampling, the pixels
            converge faster than with uniform hemisphere sampling. As shown from the rendered scenes with different light rays, lighting sampling is able to obtain a low noise level even with 1 camera ray sample per pixel.
        </p>
        <br>


        <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
        <!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

        <h3>
            Walk through your implementation of the indirect lighting function.
        </h3>
        <p>
            Indirect lighting involves radiance with more than one bounce. As a camera ray enters a scene, we trace multiple bounce paths that the ray goes through. To determine where the next ray-primitive intersection is, we find out where the light comes from.
        </p>
        <p>
            In PathTracer::est_radiance_global_illumination, return zero_bounce_radiance(r, isect) + at_least_one_bounce_radiance(r, isect) as the output.
        </p>
        <div align="middle">
            <img src="images/part4/indirect.png" align="middle" width="950px" />
        </div>
        <br>

        <h3>
            Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part4/4_global_illum_bunny.png" align="middle" width="400px" />
                        <figcaption>CBbunny.dae</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_global_illum_banana.png" align="middle" width="400px" />
                        <figcaption>banana.dae</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>

        <h3>
            Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part4/4_direct_bunny.png" align="middle" width="400px" />
                        <figcaption>Only direct illumination (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_indirect_bunny.png" align="middle" width="400px" />
                        <figcaption>Only indirect illumination (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <p>
            Direct illumination includes the zero bounce illumination and one bounce illumination, whereas the indirect illumination has more than one bounce. In the rendered view with only direct illumination, the area light at the ceiling in the scene is visible,
            as well as the shadows in the room, on and around the bunny. In comparison, in the rendered view with only indirect illumination, the area light is no longer visible, the reflection of the red and blue wall on the floor and the bunny is visible
            because there are hints of red and blue on the floor and bunny.
        </p>
        <br>

        <h3>
            For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part4/4_CBbunny_m0.png" align="middle" width="400px" />
                        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_CBbunny_m1.png" align="middle" width="400px" />
                        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part4/4_CBbunny_m2.png" align="middle" width="400px" />
                        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_CBbunny_m3.png" align="middle" width="400px" />
                        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part4/4_CBbunny_m100.png" align="middle" width="400px" />
                        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <p>
            Max_ray_depth = 0 refers to zero bounce illumination. Max_ray_depth = 1 refers to one bounce illumination. As the value of max_ray_depth increases, the shading and lighting becomes more realistics. For example, we can see the results of light bounces
            between the red wall, the floor, and bunny, the light bounces between the blue wall, the floor, and bunny. The light reflection is most obvious when max_ray_depth = 100. In conclusion, the more recursive calls to at_least_one_bounce_radiance,
            the more realistic the rendered scene is as the output radiance includes light bouncing from multiple different diretcions, thus the shading on a surface is more vivid with respect to the lighting and colors of the scene.
        </p>
        <br>

        <h3>
            Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part4/4_bunny_l1.png" align="middle" width="400px" />
                        <figcaption>1 sample per pixel (example1.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_bunny_l2.png" align="middle" width="400px" />
                        <figcaption>2 samples per pixel (example1.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part4/4_bunny_l4.png" align="middle" width="400px" />
                        <figcaption>4 samples per pixel (example1.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_bunny_l8.png" align="middle" width="400px" />
                        <figcaption>8 samples per pixel (example1.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part4/4_bunny_l16.png" align="middle" width="400px" />
                        <figcaption>16 samples per pixel (example1.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part4/4_bunny_l64.png" align="middle" width="400px" />
                        <figcaption>64 samples per pixel (example1.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part4/4_bunny_l1024.png" align="middle" width="400px" />
                        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <br>
        <p>
            With max_ray_depth of 3, the scene with spheres at different samples per pixels are shown above. The higher the sample rate, the lower the noise in the rendered scene. With 1 sample per pixel, the artifacts are very obvious, but with 1024 samples per
            pixel, the noise is almost unnoticable. Having more samples also helps with anti-aliasing.
        </p>
        <br>


        <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
        <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

        <h3>
            Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
        </h3>
        <p>
            Adaptive sampling reduces the number of samples for pixels that converge faster with low sampling rates, while allowing pixels that converge with higher sampling rates to still have a specified total number of samples. This speeds up computation by not
            sampling and tracing as many camera rays through the scene while maintaining low noises for the rendered images.
        </p>

        <p>
            Modify PathTracer::raytrace_pixel as follows:
        </p>

        <div align="middle">
            <img src="images/part5/adapt.png" align="middle" width="950px" />
        </div>

        <br>

        <h3>
            Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive
            sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
        </h3>
        <!-- Example of including multiple figures -->
        <div align="middle">
            <table style="width:100%">
                <tr align="center">
                    <td>
                        <img src="images/part5/banana_adap.png" align="middle" width="400px" />
                        <figcaption>RendeIred image (banana.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part5/banana_adap_rate.png" align="middle" width="400px" />
                        <figcaption>Sample rate image (banana.dae)</figcaption>
                    </td>
                </tr>
                <tr align="center">
                    <td>
                        <img src="images/part5/CBbunny_adap.png" align="middle" width="400px" />
                        <figcaption>Rendered image (CBbunny.dae)</figcaption>
                    </td>
                    <td>
                        <img src="images/part5/CBbunny_adap_rate.png" align="middle" width="400px" />
                        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
                    </td>
                </tr>
            </table>
        </div>
        <p>
            As shown from the sampling rate images above, the sampling rates are different across different regions and pixels of the scene. For example, in areas of shadows, the sampling rate is high, whereas in areas of almost uniform colors, the sampling rate
            is low.
        </p>
        <br>


</body>

</html>